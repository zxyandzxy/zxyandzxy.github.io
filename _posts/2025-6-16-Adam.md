---
layout: post
title: "Adam优化算法"
date: 2025-06-16
tags: [LLM]
comments: true
author: zxy
---

> 参考文档：[一篇入门之-Adam算法（含原理、计算公式逐行解读、实现代码）-老饼讲解](https://www.bbbdata.com/text/700)

## AdaGrad算法

AdaGrad全称为自适应梯度下降算法（Adaptive Gradient Algorithm）,是梯度下降法的一种改进在一般的优化算法中，学习率都是统一的，而AdaGrad算法则为每个参数自动调整学习率

### AdaGrad算法介绍   

我们记待优化的第i个参数为$x_i$，AdaGrad算法的更新公式如下： $$ s_i = s_i + g_i^2 $$ $$ x_i = x_i - \frac{\eta}{\sqrt{s_i + \epsilon}} g_i $$ 

- $g_i$: 参数$x_i$的梯度 
- $s_i$: 初始值为0，它实际就是$g_i$的累计平方和 
-  $\eta$: 学习率 
-  $\epsilon$: 一个极小的常数，它的作用避免分母为0 

可以看到，AdaGrad就是在梯度下降算法的基础上通过将 学习率$\eta$除以$\sqrt{s_i + \epsilon}$ 来对每一个参数的学习率进行自动调整。 

### AdaGrad算法-优缺点

 AdaGrad最大的优点是，它可以一定程度上解决梯度消失引起的训练困难问题。在深度学习中，浅层的参数往往会因为梯度消失而难以调整，而AdaGrad加入了自适应学习率，梯度小的参数会获得更大的学习率，从而缓解了梯度消失问题。 

AdaGrad最大的缺点是，学习率会持续缩小，导致在一定步数后学习率会极小，导致无法继续学习。可以看到，AdaGrad中的$s_i$就是$g_i$的累计平方和，在一定步数后，$s_i$大到一定程度时，学习率$\frac{\eta}{\sqrt{s_i + \epsilon}}$就会变得极小，从而无法继续学习。



## RMSProp算法

RMSProp算法（均方根传播，Root Mean Square Propagation‌）是2012年提出的一种优化算法它是对AdaGrad算法的一种优化与改进，旨在解决AdaGrad算法后期学习率消失的问题

记待优化的第i个参数为$x_i$，RMSProp算法的更新公式如下： $$ s_i = \gamma * s_i + (1 - \gamma) * g_i^2 $$          $$ x_i = x_i - \frac{\eta}{\sqrt{s_i + \epsilon}} g_i $$ 

- $g_i$: 参数$x_i$的梯度 
- $\gamma$: 衰减系数，取值范围为[0,1] 
- $s_i$: 初始值为0，它实际就是$g_i$的累计平方 
- $\eta$: 学习率 
- $\epsilon$: 一个极小的常数，它的作用避免分母为0 

可以看到，RMSProp就是在AdaGrad算法的基础上，对$s_i$的更新公式进行了修改。RMSProp通过引入衰减系数$\gamma$来避免$s_i$的无限增大，从而解决AdaGrad算法中学习率消失的问题。



## 动量梯度下降法

动量梯度下降法是梯度下降法的一种改进，它加入了动量机制来达到跳出局部最优。为了能够跳出局部最优，动量梯度下降法借鉴了物体从高处滚到低处的原理，由于物体下坡时具有动量，遇到小坑时会由于原有动量的推动，从而跃出小坑。 因此，动量梯度下降法在迭代的过程中引入动量的概念。 

动量梯度下降法迭代公式如下： $$ v_t = mc * v_{t-1} + (1 - mc) * (-lr * g) $$                 $$ x_t = x_{t-1} + v_t $$ 

$mc$: 动量系数，一般设为0.9          $g$: 负梯度 

解释：公式整体的意思是，将“速度”作为X的迭代量，而负梯度则作为速度的修改量。其中，$mc$是原速度的权重，$(1-mc)$是加速度（负梯度）的权重。 这样做的好处是，在遇到“小坑”的时候，会保持原有的速度方向，冲出小坑。



## Adam算法

Adam算法（Adaptive Moment Estimation）是目前深度学习中最常用的一种优化算法它没有太多新内容，只是简单地整合了RMProp算法与动量梯度下降法，从而得到的一种效果更佳的优化算法。Adam算法的特点是，既拥有RMSProp的自适应学习率，同时又能像动量法那样，跳出局部最优。 

记待优化的第i个参数为$x_i$，Adam算法的更新公式如下：

1.  $$ v_i = \beta_1 * v_i + (1 - \beta_1) * g_i \tag{1} $$ 
2. $$ s_i = \beta_2 * s_i + (1 - \beta_2) * g_i^2 \tag{2} $$ 
3. $$ \hat{v}_i = \frac{v_i}{1 - \beta_1^t} \tag{3} $$ 
4. $$ \hat{s}_i = \frac{s_i}{1 - \beta_2^t} \tag{4} $$ 
5. $$ x_i = x_i - \frac{\eta}{\sqrt{\hat{s}_i} + \epsilon} \hat{v}_i \tag{5} $$ 

- $g_i$: 参数$x_i$的梯度 
- $v_i$: $x_i$的速度，初始值为0 
- $s_i$: $g_i$的累计平方和，初始值为0 
- $\beta_1$: $v$的衰减系数，取值范围为[0,1]，一般设为0.9 
- $\beta_2$: $s$的衰减系数，取值范围为[0,1]，一般设为0.999 
- $t$: 当前的迭代次数 
- $\eta$: 学习率 
- $\epsilon$: 一个极小的常数，它的作用避免分母为0

参考RMSProp算法与动量梯度下降法就能理解Adam算法了：(1) 式参考动量梯度下降法，它引入速度$v$来作为$x$更新时的迭代量。 (2) 式参考RMSProp算法，引入累计平方和来自适应地调整学习率。 (3)，(4) 式则是对$v$、$s$的修正。 主要是在迭代初期，$v$、$s$会偏小，因此对它们进行修正。当迭代次数$t$达到一定次数后，$\beta^t$会近似于0，此时$\hat{v}$、$\hat{s}$就近似于$v$、$s$了。 (5) 式用于更新参数$x$。 它参考RMSProp算法，使用$\sqrt{\hat{s}_i} + \epsilon$来调整学习率。同时参考动量梯度下降法，用$\hat{v}$来替代梯度$g$作为迭代量。 总的来说，Adam算法就是引入了动量法中的速度作为更新量，并采用了RMSProp中的自适应学习率，并加上了一点小技巧，对$v$、$s$作了一点小修正（事实上修不修正都没多大关系，毕竟影响的只有前面的几次迭代）。
