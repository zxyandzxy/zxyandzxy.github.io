---
layout: post
title: "自然语言处理：预训练"
date: 2024-9-20
tags: [deep-learning]
comments: true
author: zxy
---

## 词嵌入(word2vec)

[14.1. 词嵌入（word2vec） — 动手学深度学习 2.0.0 documentation (d2l.ai)](https://zh-v2.d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html)



## 近似训练

[14.2. 近似训练 — 动手学深度学习 2.0.0 documentation (d2l.ai)](https://zh-v2.d2l.ai/chapter_natural-language-processing-pretraining/approx-training.html)



## 用于预训练词嵌入的数据集

[14.3. 用于预训练词嵌入的数据集 — 动手学深度学习 2.0.0 documentation (d2l.ai)](https://zh-v2.d2l.ai/chapter_natural-language-processing-pretraining/word-embedding-dataset.html)



## 预训练word2vec

[14.4. 预训练word2vec — 动手学深度学习 2.0.0 documentation (d2l.ai)](https://zh-v2.d2l.ai/chapter_natural-language-processing-pretraining/word2vec-pretraining.html)



## 全局向量的词嵌入(Glove)

[14.5. 全局向量的词嵌入（GloVe） — 动手学深度学习 2.0.0 documentation (d2l.ai)](https://zh-v2.d2l.ai/chapter_natural-language-processing-pretraining/glove.html)



## 子词嵌入

[14.6. 子词嵌入 — 动手学深度学习 2.0.0 documentation (d2l.ai)](https://zh-v2.d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html)



## 词的相似性和类比任务

[14.7. 词的相似性和类比任务 — 动手学深度学习 2.0.0 documentation (d2l.ai)](https://zh-v2.d2l.ai/chapter_natural-language-processing-pretraining/similarity-analogy.html)



## 来自Transformers的双向编码器表示(BERT)

[14.8. 来自Transformers的双向编码器表示（BERT） — 动手学深度学习 2.0.0 documentation (d2l.ai)](https://zh-v2.d2l.ai/chapter_natural-language-processing-pretraining/bert.html)



## 用于预训练BERT的数据集

[14.9. 用于预训练BERT的数据集 — 动手学深度学习 2.0.0 documentation (d2l.ai)](https://zh-v2.d2l.ai/chapter_natural-language-processing-pretraining/bert-dataset.html)



## 预训练BERT

[14.10. 预训练BERT — 动手学深度学习 2.0.0 documentation (d2l.ai)](https://zh-v2.d2l.ai/chapter_natural-language-processing-pretraining/bert-pretraining.html)