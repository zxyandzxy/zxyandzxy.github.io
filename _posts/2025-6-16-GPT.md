---
layout: post
title: "看看Transformer和GPT的底层"
date: 2025-06-16
tags: [LLM]
comments: true
author: zxy
---

参考资料：

（1）[Transformers (how LLMs work) explained visually | DL5](https://www.youtube.com/watch?v=wjZofJX0v4M)

以GPT为例讲解底层涉及到的一些基础原理（词嵌入、注意力机制、softmax以及temperature） + GPT是如何进行工作的（本质上是一个以transformer架构为基础的预测下一个单词的模型、GPT的超大量参数可以分为几类（embedding，unembedding， key, query, ....，每一类的作用是什么）

（2）[Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)

作者从0开始构建了一个极简版GPT。具体来说，作者采用一种循序渐进的方式逐步构建一个类似于GPT的文本输出模型

1. 讲解数据集是什么，整体来说需要什么架构与流程（编码器，解码器，模型，训练拟合，测试）
2. 讲解最简单的bigram模型，如何搭建，如何训练测试
3. 引入注意力机制（生成式模型只关注前文，这个目标如何实现，有三种方法，最后一种对应了注意力机制的本质。通过权重表达这个位置的token对于其他位置token的关系程度）